{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMicfMWoGAfvb9d//8GO5cM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parth-kalkotwar/AuthSample/blob/master/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUA3aIdfu4i3"
      },
      "source": [
        "import gensim\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from gensim.models import Word2Vec\r\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\r\n",
        "from io import BytesIO\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from urllib.request import urlopen\r\n",
        "from zipfile import ZipFile\r\n",
        "import numpy as np\r\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygNuFNY7wJVN"
      },
      "source": [
        "# Arguments\r\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngeijNHEwPNR",
        "outputId": "9e48d7c5-fce2-4768-f1c4-d5451fe1094e"
      },
      "source": [
        "# Unzip the file (may take ~3-5 minutes)\r\n",
        "resp = urlopen('http://nlp.stanford.edu/data/glove.6B.zip')\r\n",
        "zipfile = ZipFile(BytesIO(resp.read()))\r\n",
        "zipfile.namelist()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['glove.6B.50d.txt',\n",
              " 'glove.6B.100d.txt',\n",
              " 'glove.6B.200d.txt',\n",
              " 'glove.6B.300d.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oRarq_inwXwV",
        "outputId": "38c66c5c-9cce-4d67-8dcb-6b38437dd7cf"
      },
      "source": [
        "embeddings_file = f\"glove.6B.{EMBEDDING_DIM}d.txt\"\r\n",
        "zipfile.extract(embeddings_file)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/glove.6B.100d.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx0FZmCMwwda",
        "outputId": "69acb367-d412-440f-c4c0-fe8d9a2b563b"
      },
      "source": [
        "with open(embeddings_file,'r') as fp:\r\n",
        "  line = next(fp)\r\n",
        "  word = line.split()[0]\r\n",
        "  embeddings = np.asarray(line.split()[1:],dtype='float32')\r\n",
        "  print(word)\r\n",
        "  print(embeddings)\r\n",
        "  print(len(embeddings))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the\n",
            "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
            "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
            " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
            " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
            " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
            "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
            "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
            " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
            "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
            "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
            "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
            " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
            " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
            " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
            "  0.8278    0.27062 ]\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CarGPDg7xr7Z",
        "outputId": "72c7f105-2826-43f0-ba53-f63b986d993a"
      },
      "source": [
        "word2vec_output_file = f\"{embeddings_file}.word2vec\"\r\n",
        "glove2word2vec(embeddings_file,word2vec_output_file)\r\n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O47YuweyYLU"
      },
      "source": [
        "glove = KeyedVectors.load_word2vec_format(word2vec_output_file,binary=False)\r\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QN2azy2yfdd",
        "outputId": "3d1e6d3b-671d-4684-9d81-9d9f69568487"
      },
      "source": [
        "glove.most_similar(positive=['woman','king'],negative=['man'],topn = 5)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7698541283607483),\n",
              " ('monarch', 0.6843380928039551),\n",
              " ('throne', 0.6755735874176025),\n",
              " ('daughter', 0.6594556570053101),\n",
              " ('princess', 0.6520534753799438)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeBwaP0kyxhm",
        "outputId": "3ad2fc67-66b6-4219-af3c-6dff52f13b65"
      },
      "source": [
        "X = glove[glove.wv.vocab]\r\n",
        "pca = PCA(n_components=2)\r\n",
        "pca_results = pca.fit_transform(X)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq_bdCTyzfRD",
        "outputId": "e0b545d9-60ad-45a5-b7e0-0c8ad803c596"
      },
      "source": [
        "glove.most_similar(positive=['doctor','woman'],negative=['man'],topn=5)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('nurse', 0.7735227346420288),\n",
              " ('physician', 0.7189429998397827),\n",
              " ('doctors', 0.6824328303337097),\n",
              " ('patient', 0.6750682592391968),\n",
              " ('dentist', 0.6726033687591553)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xBBaH7_zqw_"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import random\r\n",
        "import torch\r\n",
        "import torch.nn as nn"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txsTyj6fzxGA",
        "outputId": "09943138-e441-405c-83ce-8835508b0d97"
      },
      "source": [
        "SEED = 1234\r\n",
        "\r\n",
        "def set_seeds(seed=1234):\r\n",
        "    \"\"\"Set seeds for reproducability.\"\"\"\r\n",
        "    np.random.seed(seed)\r\n",
        "    random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed_all(seed) # multi-GPU\r\n",
        "\r\n",
        "set_seeds(SEED)\r\n",
        "\r\n",
        "cuda = True\r\n",
        "device = torch.device('cuda' if (torch.cuda.is_available() and cuda) else 'cpu')\r\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\r\n",
        "if device.type == 'cuda':\r\n",
        "  torch.set_default_tensor_type('torch.cuda.FloatTensor')\r\n",
        "print(device)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "uBosrtZL1q9J",
        "outputId": "4e810a25-dcef-4110-9729-2bc57a62cc05"
      },
      "source": [
        "# Load data\r\n",
        "url = \"https://raw.githubusercontent.com/GokuMohandas/madewithml/main/datasets/news.csv\"\r\n",
        "df = pd.read_csv(url, header=0) # load\r\n",
        "df = df.sample(frac=1).reset_index(drop=True) # shuffle\r\n",
        "df.head()"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sharon Accepts Plan to Reduce Gaza Army Operat...</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Internet Key Battleground in Wildlife Crime Fight</td>\n",
              "      <td>Sci/Tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>July Durable Good Orders Rise 1.7 Percent</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Growing Signs of a Slowing on Wall Street</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The New Faces of Reality TV</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  category\n",
              "0  Sharon Accepts Plan to Reduce Gaza Army Operat...     World\n",
              "1  Internet Key Battleground in Wildlife Crime Fight  Sci/Tech\n",
              "2          July Durable Good Orders Rise 1.7 Percent  Business\n",
              "3          Growing Signs of a Slowing on Wall Street  Business\n",
              "4                        The New Faces of Reality TV     World"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCFZbWvA1tC6",
        "outputId": "bb928b2a-39be-4bd6-e1d0-e37a0880ff2b"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "import re\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "STOPWORDS = stopwords.words('english')\r\n",
        "print (STOPWORDS[:5])\r\n",
        "porter = PorterStemmer()\r\n",
        "\r\n",
        "def preprocess(text, stopwords=STOPWORDS):\r\n",
        "    \"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\r\n",
        "    # Lower\r\n",
        "    text = text.lower()\r\n",
        "\r\n",
        "    # Remove stopwords\r\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\r\n",
        "    text = pattern.sub('', text)\r\n",
        "\r\n",
        "    # Remove words in paranthesis\r\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)\r\n",
        "\r\n",
        "    # Spacing and filters\r\n",
        "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\r\n",
        "    text = re.sub('[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric chars\r\n",
        "    text = re.sub(' +', ' ', text)  # remove multiple spaces\r\n",
        "    text = text.strip()\r\n",
        "\r\n",
        "    return text\r\n",
        "\r\n",
        "# Apply to dataframe\r\n",
        "preprocessed_df = df.copy()\r\n",
        "preprocessed_df.title = preprocessed_df.title.apply(preprocess)\r\n",
        "print (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['i', 'me', 'my', 'myself', 'we']\n",
            "Sharon Accepts Plan to Reduce Gaza Army Operation, Haaretz Says\n",
            "\n",
            "sharon accepts plan reduce gaza army operation haaretz says\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbswEKq_13al",
        "outputId": "f6173814-1e25-46aa-d8b5-eb749244c74f"
      },
      "source": [
        "import collections\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "TRAIN_SIZE = 0.7\r\n",
        "VAL_SIZE = 0.15\r\n",
        "TEST_SIZE = 0.15\r\n",
        "\r\n",
        "def train_val_test_split(X, y, train_size):\r\n",
        "    \"\"\"Split dataset into data splits.\"\"\"\r\n",
        "    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\r\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\r\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\r\n",
        "\r\n",
        "# Data\r\n",
        "X = preprocessed_df[\"title\"].values\r\n",
        "y = preprocessed_df[\"category\"].values\r\n",
        "\r\n",
        "# Create data splits\r\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\r\n",
        "    X=X, y=y, train_size=TRAIN_SIZE)\r\n",
        "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\r\n",
        "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\r\n",
        "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\r\n",
        "print (f\"Sample point: {X_train[0]} → {y_train[0]}\")"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: (84000,), y_train: (84000,)\n",
            "X_val: (18000,), y_val: (18000,)\n",
            "X_test: (18000,), y_test: (18000,)\n",
            "Sample point: china battles north korea nuclear talks → World\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FqH2yOf2A6k",
        "outputId": "a26d6911-6801-46c5-abcd-2ebdd9b579c2"
      },
      "source": [
        "import itertools\r\n",
        "class LabelEncoder(object):\r\n",
        "    \"\"\"Label encoder for tag labels.\"\"\"\r\n",
        "    def __init__(self, class_to_index={}):\r\n",
        "        self.class_to_index = class_to_index\r\n",
        "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\r\n",
        "        self.classes = list(self.class_to_index.keys())\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.class_to_index)\r\n",
        "\r\n",
        "    def __str__(self):\r\n",
        "        return f\"<LabelEncoder(num_classes={len(self)})>\"\r\n",
        "\r\n",
        "    def fit(self, y):\r\n",
        "        classes = np.unique(y)\r\n",
        "        for i, class_ in enumerate(classes):\r\n",
        "            self.class_to_index[class_] = i\r\n",
        "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\r\n",
        "        self.classes = list(self.class_to_index.keys())\r\n",
        "        return self\r\n",
        "\r\n",
        "    def encode(self, y):\r\n",
        "        encoded = np.zeros((len(y)), dtype=int)\r\n",
        "        for i, item in enumerate(y):\r\n",
        "            encoded[i] = self.class_to_index[item]\r\n",
        "        return encoded\r\n",
        "\r\n",
        "    def decode(self, y):\r\n",
        "        classes = []\r\n",
        "        for i, item in enumerate(y):\r\n",
        "            classes.append(self.index_to_class[item])\r\n",
        "        return classes\r\n",
        "\r\n",
        "    def save(self, fp):\r\n",
        "        with open(fp, 'w') as fp:\r\n",
        "            contents = {'class_to_index': self.class_to_index}\r\n",
        "            json.dump(contents, fp, indent=4, sort_keys=False)\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def load(cls, fp):\r\n",
        "        with open(fp, 'r') as fp:\r\n",
        "            kwargs = json.load(fp=fp)\r\n",
        "        return cls(**kwargs)\r\n",
        "\r\n",
        "# Encode\r\n",
        "label_encoder = LabelEncoder()\r\n",
        "label_encoder.fit(y_train)\r\n",
        "NUM_CLASSES = len(label_encoder)\r\n",
        "label_encoder.class_to_index\r\n",
        "\r\n",
        "# Convert labels to tokens\r\n",
        "print (f\"y_train[0]: {y_train[0]}\")\r\n",
        "y_train = label_encoder.encode(y_train)\r\n",
        "y_val = label_encoder.encode(y_val)\r\n",
        "y_test = label_encoder.encode(y_test)\r\n",
        "print (f\"y_train[0]: {y_train[0]}\")\r\n",
        "\r\n",
        "# Class weights\r\n",
        "counts = np.bincount(y_train)\r\n",
        "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\r\n",
        "print (f\"counts: {counts}\\nweights: {class_weights}\")"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train[0]: World\n",
            "y_train[0]: 3\n",
            "counts: [21000 21000 21000 21000]\n",
            "weights: {0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JRsf52J2QZP"
      },
      "source": [
        "import json\r\n",
        "from collections import Counter\r\n",
        "from more_itertools import take\r\n",
        "\r\n",
        "class Tokenizer(object):\r\n",
        "  def __init__(self,char_level,num_tokens = None,pad_token=\"<PAD>\",oov_token=\"<UNK>\",token_to_index = None):\r\n",
        "    self.char_level = char_level\r\n",
        "    self.separator = '' if self.char_level else ' '\r\n",
        "    if num_tokens:\r\n",
        "      num_tokens -= 2\r\n",
        "    self.num_tokens = num_tokens\r\n",
        "    self.pad_token = pad_token\r\n",
        "    self.oov_token = oov_token\r\n",
        "    if not token_to_index:\r\n",
        "      token_to_index = {self.oov_token:1,self.pad_token:0}\r\n",
        "    self.token_to_index = token_to_index\r\n",
        "    self.index_to_token = {v:k for k,v in self.token_to_index.items()}\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.token_to_index)\r\n",
        "\r\n",
        "  def fit_on_texts(self,texts):\r\n",
        "    if not self.char_level:\r\n",
        "      texts = [text.split(\" \") for text in texts]\r\n",
        "    print(\"1. \",texts[:5])\r\n",
        "    all_tokens = [token for text in texts for token in text]\r\n",
        "    print(\"2. \",all_tokens[:5])\r\n",
        "    counts = Counter(all_tokens).most_common(self.num_tokens)\r\n",
        "    self.min_token_freq = counts[-1][1]\r\n",
        "    for token,count in counts:\r\n",
        "      index = len(self)\r\n",
        "      self.token_to_index[token] = index\r\n",
        "      self.index_to_token[index] = token\r\n",
        "    return self\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    def fit_on_texts(self, texts):\r\n",
        "        if not self.char_level:\r\n",
        "            texts = [text.split(\" \") for text in texts]\r\n",
        "        all_tokens = [token for text in texts for token in text]\r\n",
        "        counts = Counter(all_tokens).most_common(self.num_tokens)\r\n",
        "        self.min_token_freq = counts[-1][1]\r\n",
        "        for token, count in counts:\r\n",
        "            index = len(self)\r\n",
        "            self.token_to_index[token] = index\r\n",
        "            self.index_to_token[index] = token\r\n",
        "        return self\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "  def texts_to_sequences(self,texts):\r\n",
        "    sequences = []\r\n",
        "    for text in texts:\r\n",
        "      if not self.char_level:\r\n",
        "        text = text.split(\" \")\r\n",
        "      sequence = []\r\n",
        "      for token in text:\r\n",
        "        sequence.append(self.token_to_index.get(token,self.token_to_index[self.oov_token]))\r\n",
        "      sequences.append(np.asarray(sequence))\r\n",
        "    return sequences\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    sequences = []\r\n",
        "        for text in texts:\r\n",
        "            if not self.char_level:\r\n",
        "                text = text.split(' ')\r\n",
        "            sequence = []\r\n",
        "            for token in text:\r\n",
        "                sequence.append(self.token_to_index.get(\r\n",
        "                    token, self.token_to_index[self.oov_token]))\r\n",
        "            sequences.append(np.asarray(sequence))\r\n",
        "        return sequences\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "  def sequences_to_texts(self,sequences):\r\n",
        "    texts = []\r\n",
        "    for sequence in sequences:\r\n",
        "      text = []\r\n",
        "      for index in sequence:\r\n",
        "        text.append(self.index_to_token.get(index,self.oov_token))\r\n",
        "      texts.append(self.separator.join([token for token in text]))\r\n",
        "    return texts\r\n",
        "\r\n",
        "    def save(self, fp):\r\n",
        "        with open(fp, 'w') as fp:\r\n",
        "            contents = {\r\n",
        "                'char_level': self.char_level,\r\n",
        "                'oov_token': self.oov_token,\r\n",
        "                'token_to_index': self.token_to_index\r\n",
        "            }\r\n",
        "            json.dump(contents, fp, indent=4, sort_keys=False)\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def load(cls, fp):\r\n",
        "        with open(fp, 'r') as fp:\r\n",
        "            kwargs = json.load(fp=fp)\r\n",
        "        return cls(**kwargs)\r\n",
        "\r\n"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeX1rdtQ7aB5",
        "outputId": "c1519d9f-8726-4f77-f1ac-f2149688b913"
      },
      "source": [
        "tokenizer = Tokenizer(char_level=False, num_tokens=5000)\r\n",
        "tokenizer.fit_on_texts(texts=X_train)\r\n",
        "VOCAB_SIZE = len(tokenizer)\r\n",
        "print (tokenizer)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.  [['china', 'battles', 'north', 'korea', 'nuclear', 'talks'], ['ncaa', 'game', 'summary', 'colorado', 'nebraska'], ['day', '2', 'kobe', 'bryant', 'jury', 'selection'], ['champions', 'trophy', 'win', 'may', 'herald', 'new', 'era', 'wi', 'lara'], ['babson', 'finishes', 'strong', 'reach', 'ncaa', 'tourney']]\n",
            "2.  ['china', 'battles', 'north', 'korea', 'nuclear']\n",
            "<__main__.Tokenizer object at 0x7fbaf8dea290>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7xV-xql7xaK",
        "outputId": "303b3c8d-9163-44fe-a85d-1a2e53f7bbaf"
      },
      "source": [
        "# Sample of tokens\r\n",
        "print (take(5, tokenizer.token_to_index.items()))\r\n",
        "print (f\"least freq token's freq: {tokenizer.min_token_freq}\") # use this to adjust num_tokens"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('<UNK>', 1), ('<PAD>', 0), ('39', 2), ('b', 3), ('gt', 4)]\n",
            "least freq token's freq: 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUhbPvSu72z0",
        "outputId": "8053e40b-6998-4230-8739-d36cc995a3af"
      },
      "source": [
        "# Convert texts to sequences of indices\r\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\r\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\r\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\r\n",
        "preprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\r\n",
        "print (\"Text to indices:\\n\"\r\n",
        "    f\"  (preprocessed) → {preprocessed_text}\\n\"\r\n",
        "    f\"  (tokenized) → {X_train[0]}\")"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text to indices:\n",
            "  (preprocessed) → china battles north korea nuclear talks\n",
            "  (tokenized) → [  16 1491  285  142  114   24]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMo5EApl72uC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff8cecb1-de97-4457-d532-b86b674aa53c"
      },
      "source": [
        "vocab_size = 10\r\n",
        "x = torch.randint(high=vocab_size, size=(1,5))\r\n",
        "print (x)\r\n",
        "print (x.shape)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2, 6, 5, 2, 6]])\n",
            "torch.Size([1, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH5KmY1F_LN2",
        "outputId": "66fa8635-6f8f-40ea-9ff1-a417e3e058af"
      },
      "source": [
        "embeddings = nn.Embedding(embedding_dim=100, num_embeddings=vocab_size)\r\n",
        "print (embeddings.weight.shape)\r\n",
        "embeddings(x).shape"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI_MkQEB_gXy"
      },
      "source": [
        "def pad_sequences(sequences, max_seq_len=0):\r\n",
        "    \"\"\"Pad sequences to max length in sequence.\"\"\"\r\n",
        "    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\r\n",
        "    padded_sequences = np.zeros((len(sequences), max_seq_len))\r\n",
        "    for i, sequence in enumerate(sequences):\r\n",
        "        padded_sequences[i][:len(sequence)] = sequence\r\n",
        "    return padded_sequences"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK3S3Pld_kYS",
        "outputId": "18e99880-8b54-43fa-fefa-64377444e0eb"
      },
      "source": [
        "padded = pad_sequences(X_train[0:3])\r\n",
        "print (padded.shape)\r\n",
        "print (padded)"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 6)\n",
            "[[1.600e+01 1.491e+03 2.850e+02 1.420e+02 1.140e+02 2.400e+01]\n",
            " [1.445e+03 2.300e+01 6.560e+02 2.197e+03 1.000e+00 0.000e+00]\n",
            " [1.200e+02 1.400e+01 1.955e+03 1.005e+03 1.529e+03 4.014e+03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpi9SlC5BgsE",
        "outputId": "0eb61b8c-adf1-4c61-9369-cae75ae78b19"
      },
      "source": [
        "FILTER_SIZES = list(range(1, 4))\r\n",
        "\r\n",
        "class Dataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, X, y, max_filter_size):\r\n",
        "        self.X = X\r\n",
        "        self.y = y\r\n",
        "        self.max_filter_size = max_filter_size\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.y)\r\n",
        "\r\n",
        "    def __str__(self):\r\n",
        "        return f\"<Dataset(N={len(self)})>\"\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        X = self.X[index]\r\n",
        "        y = self.y[index]\r\n",
        "        return [X, y]\r\n",
        "\r\n",
        "    def collate_fn(self, batch):\r\n",
        "        \"\"\"Processing on a batch.\"\"\"\r\n",
        "        # Get inputs\r\n",
        "        batch = np.array(batch, dtype=object)\r\n",
        "        X = batch[:, 0]\r\n",
        "        y = np.stack(batch[:, 1], axis=0)\r\n",
        "\r\n",
        "        # Pad sequences\r\n",
        "        X = pad_sequences(X)\r\n",
        "\r\n",
        "        # Cast\r\n",
        "        X = torch.LongTensor(X.astype(np.int32))\r\n",
        "        y = torch.LongTensor(y.astype(np.int32))\r\n",
        "\r\n",
        "        return X, y\r\n",
        "\r\n",
        "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\r\n",
        "        return torch.utils.data.DataLoader(\r\n",
        "            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\r\n",
        "            shuffle=shuffle, drop_last=drop_last, pin_memory=True)\r\n",
        "        \r\n",
        "# Create datasets\r\n",
        "max_filter_size = max(FILTER_SIZES)\r\n",
        "train_dataset = Dataset(X=X_train, y=y_train, max_filter_size=max_filter_size)\r\n",
        "val_dataset = Dataset(X=X_val, y=y_val, max_filter_size=max_filter_size)\r\n",
        "test_dataset = Dataset(X=X_test, y=y_test, max_filter_size=max_filter_size)\r\n",
        "print (\"Datasets:\\n\"\r\n",
        "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\r\n",
        "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\r\n",
        "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\r\n",
        "    \"Sample point:\\n\"\r\n",
        "    f\"  X: {train_dataset[0][0]}\\n\"\r\n",
        "    f\"  y: {train_dataset[0][1]}\")"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datasets:\n",
            "  Train dataset:<Dataset(N=84000)>\n",
            "  Val dataset: <Dataset(N=18000)>\n",
            "  Test dataset: <Dataset(N=18000)>\n",
            "Sample point:\n",
            "  X: [  16 1491  285  142  114   24]\n",
            "  y: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMYWmStQCrPM",
        "outputId": "afc5cdba-13b0-4261-dc22-327ed8af6198"
      },
      "source": [
        "# Create dataloaders\r\n",
        "batch_size = 64\r\n",
        "train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)\r\n",
        "val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)\r\n",
        "test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)\r\n",
        "batch_X, batch_y = next(iter(train_dataloader))\r\n",
        "print (\"Sample batch:\\n\"\r\n",
        "    f\"  X: {list(batch_X.size())}\\n\"\r\n",
        "    f\"  y: {list(batch_y.size())}\\n\"\r\n",
        "    \"Sample point:\\n\"\r\n",
        "    f\"  X: {batch_X[0]}\\n\"\r\n",
        "    f\"  y: {batch_y[0]}\")"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample batch:\n",
            "  X: [64, 14]\n",
            "  y: [64]\n",
            "Sample point:\n",
            "  X: tensor([  16, 1491,  285,  142,  114,   24,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0], device='cpu')\n",
            "  y: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-u--S8rDF4d"
      },
      "source": [
        "import math\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "EMBEDDING_DIM = 100\r\n",
        "HIDDEN_DIM = 100\r\n",
        "DROPOUT_P = 0.1\r\n",
        "\r\n",
        "# class CNN(nn.Module):\r\n",
        "#   def __init__(self, embedding_dim, vocab_size, num_filters,\r\n",
        "#                  filter_sizes, hidden_dim, dropout_p, num_classes,\r\n",
        "#                  pretrained_embeddings=None, freeze_embeddings=False,\r\n",
        "#                  padding_idx=0):\r\n",
        "#     super(CNN,self).__init__()\r\n",
        "\r\n",
        "#     self.filter_sizes = filter_sizes\r\n",
        "#     if pretrained_embeddings is None:\r\n",
        "#                 self.embeddings = nn.Embedding(\r\n",
        "#                 embedding_dim=embedding_dim, num_embeddings=vocab_size,\r\n",
        "#                 padding_idx=padding_idx)\r\n",
        "#     else:\r\n",
        "#             pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\r\n",
        "#             self.embeddings = nn.Embedding(\r\n",
        "#                 embedding_dim=embedding_dim, num_embeddings=vocab_size,\r\n",
        "#                 padding_idx=padding_idx, _weight=pretrained_embeddings)\r\n",
        "#     if freeze_embeddings:\r\n",
        "#       self.embeddings.weights.requires_grad = False\r\n",
        "\r\n",
        "#     self.conv = nn.ModuleList(\r\n",
        "#         [nn.Conv1d(in_channels = embedding_dim,out_channels = num_filters,kernel_size = f) for f in filter_sizes]\r\n",
        "#     )\r\n",
        "\r\n",
        "#     self.dropout = nn.Dropout(dropout_p)\r\n",
        "#     self.fc1 = nn.Linear(num_filters*len(filter_sizes),hidden_dim)\r\n",
        "#     self.fc2 = nn.Linear(hidden_dim,num_classes)\r\n",
        "\r\n",
        "#   def forward(self,inputs,channel_first = False,apply_softmax = False):\r\n",
        "#     x_in, = inputs\r\n",
        "#     x_in = self.embeddings(x_in)\r\n",
        "\r\n",
        "#     if not channel_first:\r\n",
        "#       x_in = x_in.transpose(1,2)\r\n",
        "\r\n",
        "#     z = []\r\n",
        "#     max_seq_len = x_in.shape[2]\r\n",
        "#     for i,f in enumerate(self.filter_sizes):\r\n",
        "#       padding_left = int((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2)\r\n",
        "#       padding_right = int(math.ceil((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2))\r\n",
        "#       _z = self.conv[i](F.pad(x_in,(padding_left,padding_right)))\r\n",
        "#       _z = F.max_pool1d(_z,_z.size(2)).squeeze(2)\r\n",
        "#       z.append(_z)\r\n",
        "#     z = torch.cat(z,1)\r\n",
        "#     z = self.fc1(z)\r\n",
        "#     z = self.dropout(z)\r\n",
        "#     y_pred = self.fc2(z)\r\n",
        "\r\n",
        "#     if apply_softmax == True:\r\n",
        "#       y_pred = F.softmax(y_pred,dim=1)\r\n",
        "#     return y_pred\r\n",
        "\r\n",
        "\r\n",
        "class CNN(nn.Module):\r\n",
        "    def __init__(self, embedding_dim, vocab_size, num_filters,\r\n",
        "                 filter_sizes, hidden_dim, dropout_p, num_classes,\r\n",
        "                 pretrained_embeddings=None, freeze_embeddings=False,\r\n",
        "                 padding_idx=0):\r\n",
        "        super(CNN, self).__init__()\r\n",
        "\r\n",
        "        # Filter sizes\r\n",
        "        self.filter_sizes = filter_sizes\r\n",
        "\r\n",
        "        # Initialize embeddings\r\n",
        "        if pretrained_embeddings is None:\r\n",
        "            self.embeddings = nn.Embedding(\r\n",
        "                embedding_dim=embedding_dim, num_embeddings=vocab_size,\r\n",
        "                padding_idx=padding_idx)\r\n",
        "        else:\r\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\r\n",
        "            self.embeddings = nn.Embedding(\r\n",
        "                embedding_dim=embedding_dim, num_embeddings=vocab_size,\r\n",
        "                padding_idx=padding_idx, _weight=pretrained_embeddings)\r\n",
        "\r\n",
        "        # Freeze embeddings or not\r\n",
        "        if freeze_embeddings:\r\n",
        "            self.embeddings.weight.requires_grad = False\r\n",
        "\r\n",
        "        # Conv weights\r\n",
        "        self.conv = nn.ModuleList(\r\n",
        "            [nn.Conv1d(in_channels=embedding_dim,\r\n",
        "                       out_channels=num_filters,\r\n",
        "                       kernel_size=f) for f in filter_sizes])\r\n",
        "\r\n",
        "        # FC weights\r\n",
        "        self.dropout = nn.Dropout(dropout_p)\r\n",
        "        self.fc1 = nn.Linear(num_filters*len(filter_sizes), hidden_dim)\r\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\r\n",
        "\r\n",
        "    def forward(self, inputs, channel_first=False, apply_softmax=False):\r\n",
        "\r\n",
        "        # Embed\r\n",
        "        x_in, = inputs\r\n",
        "        x_in = self.embeddings(x_in)\r\n",
        "\r\n",
        "        # Rearrange input so num_channels is in dim 1 (N, C, L)\r\n",
        "        if not channel_first:\r\n",
        "            x_in = x_in.transpose(1, 2)\r\n",
        "\r\n",
        "        # Conv outputs\r\n",
        "        z = []\r\n",
        "        max_seq_len = x_in.shape[2]\r\n",
        "        for i, f in enumerate(self.filter_sizes):\r\n",
        "            # `SAME` padding\r\n",
        "            padding_left = int((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2)\r\n",
        "            padding_right = int(math.ceil((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2))\r\n",
        "\r\n",
        "            # Conv + pool\r\n",
        "            _z = self.conv[i](F.pad(x_in, (padding_left, padding_right)))\r\n",
        "            _z = F.max_pool1d(_z, _z.size(2)).squeeze(2)\r\n",
        "            z.append(_z)\r\n",
        "\r\n",
        "        # Concat conv outputs\r\n",
        "        z = torch.cat(z, 1)\r\n",
        "\r\n",
        "        # FC layers\r\n",
        "        z = self.fc1(z)\r\n",
        "        z = self.dropout(z)\r\n",
        "        y_pred = self.fc2(z)\r\n",
        "\r\n",
        "        if apply_softmax:\r\n",
        "            y_pred = F.softmax(y_pred, dim=1)\r\n",
        "        return y_pred"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZjzB4D6K9yw",
        "outputId": "0f971c91-5436-4fc3-b4f5-8a926ac732d5"
      },
      "source": [
        "def load_glove_embeddings(embeddings_file):\r\n",
        "  embeddings = {}\r\n",
        "  with open(embeddings_file,'r') as fp:\r\n",
        "    for index,line in enumerate(fp):\r\n",
        "      values = line.split()\r\n",
        "      word = values[0]\r\n",
        "      embedding = np.asarray(values[1:], dtype='float32')\r\n",
        "      embeddings[word] = embedding\r\n",
        "  return embeddings\r\n",
        "\r\n",
        "\r\n",
        "def make_embeddings_matrix(embeddings,word_index,embedding_dim):\r\n",
        "  embedding_matrix = np.zeros((len(word_index),embedding_dim))\r\n",
        "  for word,i in word_index.items():\r\n",
        "    embedding_vector = embeddings.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "      embedding_matrix[i] = embedding_vector\r\n",
        "  return embedding_matrix\r\n",
        "\r\n",
        "# Create embeddings\r\n",
        "embeddings_file = 'glove.6B.{0}d.txt'.format(EMBEDDING_DIM)\r\n",
        "glove_embeddings = load_glove_embeddings(embeddings_file=embeddings_file)\r\n",
        "embedding_matrix = make_embeddings_matrix(\r\n",
        "    embeddings=glove_embeddings, word_index=tokenizer.token_to_index,\r\n",
        "    embedding_dim=EMBEDDING_DIM)\r\n",
        "print (f\"<Embeddings(words={embedding_matrix.shape[0]}, dim={embedding_matrix.shape[1]})>\")"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Embeddings(words=5000, dim=100)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3GTOToQMx4B"
      },
      "source": [
        "import json\r\n",
        "from sklearn.metrics import precision_recall_fscore_support\r\n",
        "from torch.optim import Adam\r\n",
        "\r\n",
        "NUM_FILTERS = 50\r\n",
        "LEARNING_RATE = 1e-3\r\n",
        "PATIENCE = 5\r\n",
        "NUM_EPOCHS = 10\r\n",
        "\r\n",
        "class Trainer(object):\r\n",
        "    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\r\n",
        "\r\n",
        "        # Set params\r\n",
        "        self.model = model\r\n",
        "        self.device = device\r\n",
        "        self.loss_fn = loss_fn\r\n",
        "        self.optimizer = optimizer\r\n",
        "        self.scheduler = scheduler\r\n",
        "\r\n",
        "    def train_step(self, dataloader):\r\n",
        "        \"\"\"Train step.\"\"\"\r\n",
        "        # Set model to train mode\r\n",
        "        self.model.train()\r\n",
        "        loss = 0.0\r\n",
        "\r\n",
        "        # Iterate over train batches\r\n",
        "        for i, batch in enumerate(dataloader):\r\n",
        "\r\n",
        "            # Step\r\n",
        "            batch = [item.to(self.device) for item in batch]  # Set device\r\n",
        "            inputs, targets = batch[:-1], batch[-1]\r\n",
        "            self.optimizer.zero_grad()  # Reset gradients\r\n",
        "            z = self.model(inputs)  # Forward pass\r\n",
        "            J = self.loss_fn(z, targets)  # Define loss\r\n",
        "            J.backward()  # Backward pass\r\n",
        "            self.optimizer.step()  # Update weights\r\n",
        "\r\n",
        "            # Cumulative Metrics\r\n",
        "            loss += (J.detach().item() - loss) / (i + 1)\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def eval_step(self, dataloader):\r\n",
        "        \"\"\"Validation or test step.\"\"\"\r\n",
        "        # Set model to eval mode\r\n",
        "        self.model.eval()\r\n",
        "        loss = 0.0\r\n",
        "        y_trues, y_probs = [], []\r\n",
        "\r\n",
        "        # Iterate over val batches\r\n",
        "        with torch.no_grad():\r\n",
        "            for i, batch in enumerate(dataloader):\r\n",
        "\r\n",
        "                # Step\r\n",
        "                batch = [item.to(self.device) for item in batch]  # Set device\r\n",
        "                inputs, y_true = batch[:-1], batch[-1]\r\n",
        "                z = self.model(inputs)  # Forward pass\r\n",
        "                J = self.loss_fn(z, y_true).item()\r\n",
        "\r\n",
        "                # Cumulative Metrics\r\n",
        "                loss += (J - loss) / (i + 1)\r\n",
        "\r\n",
        "                # Store outputs\r\n",
        "                y_prob = torch.sigmoid(z).cpu().numpy()\r\n",
        "                y_probs.extend(y_prob)\r\n",
        "                y_trues.extend(y_true.cpu().numpy())\r\n",
        "\r\n",
        "        return loss, np.vstack(y_trues), np.vstack(y_probs)\r\n",
        "\r\n",
        "    def predict_step(self, dataloader):\r\n",
        "        \"\"\"Prediction step.\"\"\"\r\n",
        "        # Set model to eval mode\r\n",
        "        self.model.eval()\r\n",
        "        y_probs = []\r\n",
        "\r\n",
        "        # Iterate over val batches\r\n",
        "        with torch.no_grad():\r\n",
        "            for i, batch in enumerate(dataloader):\r\n",
        "\r\n",
        "                # Forward pass w/ inputs\r\n",
        "                inputs, targets = batch[:-1], batch[-1]\r\n",
        "                y_prob = self.model(inputs, apply_softmax=True)\r\n",
        "\r\n",
        "                # Store outputs\r\n",
        "                y_probs.extend(y_prob)\r\n",
        "\r\n",
        "        return np.vstack(y_probs)\r\n",
        "\r\n",
        "    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\r\n",
        "        best_val_loss = np.inf\r\n",
        "        for epoch in range(num_epochs):\r\n",
        "            # Steps\r\n",
        "            train_loss = self.train_step(dataloader=train_dataloader)\r\n",
        "            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\r\n",
        "            self.scheduler.step(val_loss)\r\n",
        "\r\n",
        "            # Early stopping\r\n",
        "            if val_loss < best_val_loss:\r\n",
        "                best_val_loss = val_loss\r\n",
        "                best_model = self.model\r\n",
        "                _patience = patience  # reset _patience\r\n",
        "            else:\r\n",
        "                _patience -= 1\r\n",
        "            if not _patience:  # 0\r\n",
        "                print(\"Stopping early!\")\r\n",
        "                break\r\n",
        "\r\n",
        "            # Logging\r\n",
        "            print(\r\n",
        "                f\"Epoch: {epoch+1} | \"\r\n",
        "                f\"train_loss: {train_loss:.5f}, \"\r\n",
        "                f\"val_loss: {val_loss:.5f}, \"\r\n",
        "                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\r\n",
        "                f\"_patience: {_patience}\"\r\n",
        "            )\r\n",
        "        return best_model"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4e1atTwM7vz"
      },
      "source": [
        "def get_performance(y_true, y_pred, classes):\r\n",
        "    \"\"\"Per-class performance metrics.\"\"\"\r\n",
        "    # Get metrics\r\n",
        "    performance = {'overall': {}, 'class': {}}\r\n",
        "    metrics = precision_recall_fscore_support(y_true, y_pred)\r\n",
        "\r\n",
        "    # Overall performance\r\n",
        "    performance['overall']['precision'] = np.mean(metrics[0])\r\n",
        "    performance['overall']['recall'] = np.mean(metrics[1])\r\n",
        "    performance['overall']['f1'] = np.mean(metrics[2])\r\n",
        "    performance['overall']['num_samples'] = np.float64(np.sum(metrics[3]))\r\n",
        "\r\n",
        "    # Per-class performance\r\n",
        "    for i in range(len(classes)):\r\n",
        "        performance['class'][classes[i]] = {\r\n",
        "            \"precision\": metrics[0][i],\r\n",
        "            \"recall\": metrics[1][i],\r\n",
        "            \"f1\": metrics[2][i],\r\n",
        "            \"num_samples\": np.float64(metrics[3][i])\r\n",
        "        }\r\n",
        "\r\n",
        "    return performance"
      ],
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2cllWqTM_51",
        "outputId": "f4ef4e16-375c-43cb-b522-c03c163ca056"
      },
      "source": [
        "\r\n",
        "PRETRAINED_EMBEDDINGS = embedding_matrix\r\n",
        "FREEZE_EMBEDDINGS = False\r\n",
        "print(PRETRAINED_EMBEDDINGS,type(PRETRAINED_EMBEDDINGS))\r\n"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.1383      0.30316001 -0.11122    ...  0.10495     0.11652\n",
            "  -0.43040001]\n",
            " ...\n",
            " [-0.12296    -0.17267001 -0.16532999 ...  0.15668     0.89319003\n",
            "   0.046575  ]\n",
            " [-0.74062002  0.12625     0.29295999 ...  0.83617002  0.24503\n",
            "  -0.47361001]\n",
            " [-0.64841002 -0.25812     0.39583001 ... -0.035903   -0.19373\n",
            "   0.080034  ]] <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ3WGBiyNFWl",
        "outputId": "54c6ddbc-1bce-4b1e-db29-e87eac628fdc"
      },
      "source": [
        "model = CNN(\r\n",
        "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\r\n",
        "    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,\r\n",
        "    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,\r\n",
        "    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)\r\n",
        "model = model.to(device) # set device\r\n",
        "print (model.named_parameters)\r\n",
        "\r\n",
        "# Define Loss\r\n",
        "class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\r\n",
        "loss = nn.CrossEntropyLoss(weight=class_weights_tensor)\r\n",
        "\r\n",
        "# Define optimizer & scheduler\r\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\r\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n",
        "    optimizer, mode='min', factor=0.1, patience=3)\r\n",
        "\r\n",
        "\r\n",
        "# Trainer module\r\n",
        "trainer = Trainer(\r\n",
        "    model=model, device=device, loss_fn=loss,\r\n",
        "    optimizer=optimizer, scheduler=scheduler)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_parameters of CNN(\n",
            "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,))\n",
            "    (1): Conv1d(100, 50, kernel_size=(2,), stride=(1,))\n",
            "    (2): Conv1d(100, 50, kernel_size=(3,), stride=(1,))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc1): Linear(in_features=150, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svJVqYhsNhz2",
        "outputId": "cb6ecc3e-92bb-4c25-8d5b-ac980e1fa6a5"
      },
      "source": [
        "# Train\r\n",
        "best_model = trainer.train(\r\n",
        "    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 0.48853, val_loss: 0.44327, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 2 | train_loss: 0.39091, val_loss: 0.44089, lr: 1.00E-03, _patience: 5\n",
            "Epoch: 3 | train_loss: 0.34585, val_loss: 0.45821, lr: 1.00E-03, _patience: 4\n",
            "Epoch: 4 | train_loss: 0.30293, val_loss: 0.49795, lr: 1.00E-03, _patience: 3\n",
            "Epoch: 5 | train_loss: 0.25945, val_loss: 0.56242, lr: 1.00E-03, _patience: 2\n",
            "Epoch: 6 | train_loss: 0.21683, val_loss: 0.64466, lr: 1.00E-04, _patience: 1\n",
            "Stopping early!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR1l8nZuQk2Q",
        "outputId": "c960ab49-dd45-4ef8-81f2-e45c38186a42"
      },
      "source": [
        "# Get predictions\r\n",
        "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\r\n",
        "y_pred = np.argmax(y_prob, axis=1)\r\n",
        "\r\n",
        "# Determine performance\r\n",
        "performance = get_performance(\r\n",
        "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\r\n",
        "print (json.dumps(performance['overall'], indent=2))"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"precision\": 0.827561092502945,\n",
            "  \"recall\": 0.8271111111111111,\n",
            "  \"f1\": 0.827232098809634,\n",
            "  \"num_samples\": 18000.0\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw7H7zvUS9WP"
      },
      "source": [
        ""
      ],
      "execution_count": 251,
      "outputs": []
    }
  ]
}